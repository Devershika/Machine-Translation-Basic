#Libraries
!pip install fsspec==2023.9.2 sacrebleu evaluate rouge_score -qq

import pandas as pd
import numpy as np
import transformers
import torch
import evaluate
import sacrebleu
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset

ds = load_dataset("cfilt/iitb-english-hindi")

def df_splits(dataset,split):
  df = pd.json_normalize(dataset[split])
  return df

df_train = df_splits(ds,'train')
df_test = df_splits(ds,'test')
df_val = df_splits(ds,'validation')

df = pd.concat([df_train,df_test,df_val])
df = df[:10000]

df.head()

def tokenize(text):
  return text.lower().split()

df['en.tokens'] = df['translation.en'].apply(tokenize)
df['hi.tokens'] = df['translation.hi'].apply(tokenize)

from typing import Counter
def build_vocab(tokens):
  counter = Counter()
  for token in tokens:
    counter.update(token)
  vocab = {'<pad>':0,'<sos>':1,'<eos>':2,'<unk>':3}
  for word in counter:
    vocab[word] = len(vocab)
  return vocab

eng_vocab = build_vocab(df['en.tokens'])
hin_vocab = build_vocab(df['hi.tokens'])

len(eng_vocab),len(hin_vocab)

#Create dataset
class TranslationDataset(Dataset):
  def __init__(self,dataframe,eng_vocab,hin_vocab):
    self.data = dataframe
    self.eng_vocab = eng_vocab
    self.hin_vocab = hin_vocab

  def __len__(self):
    return len(self.data)

  def encode_sentence(self,tokens,vocab,eos=True):
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    if eos:
      ids.append(vocab['<eos>'])
    return ids

  def __getitem__(self,idx):
    eng_tokens = self.data.iloc[idx]['en.tokens']
    hin_tokens = self.data.iloc[idx]['hi.tokens']
    eng_ids = self.encode_sentence(eng_tokens,self.eng_vocab)
    hin_ids = [self.hin_vocab['<sos>']]+self.encode_sentence(hin_tokens,self.hin_vocab)
    return torch.tensor(eng_ids),torch.tensor(hin_ids)

def collate_fn(batch):
  eng_batch,hin_batch = zip(*batch)
  eng_padded = nn.utils.rnn.pad_sequence(eng_batch,batch_first=True,padding_value=0)
  hin_padded = nn.utils.rnn.pad_sequence(hin_batch,batch_first=True,padding_value=0)
  return eng_padded,hin_padded

df = df.sample(frac=1, random_state=2).reset_index(drop=True)
total = len(df)
train_end = int(0.9*total)
val_end = int(0.95*total)

train_loader = DataLoader(TranslationDataset(df.iloc[:train_end], eng_vocab, hin_vocab),
                          batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(TranslationDataset(df.iloc[train_end:val_end], eng_vocab, hin_vocab),
                        batch_size=2, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(TranslationDataset(df.iloc[val_end:], eng_vocab, hin_vocab),
                         batch_size=2, shuffle=False, collate_fn=collate_fn)

#Seq2Seq model
class Seq2Seq(nn.Module):
  def __init__(self, input_dim, output_dim, emb_dim=64, hidden_dim=128):
    super(Seq2Seq, self).__init__()
    self.encoder_embed = nn.Embedding(input_dim, emb_dim)
    self.encoder_lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)

    self.decoder_embed = nn.Embedding(output_dim, emb_dim)
    self.decoder_lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
    self.fc_out = nn.Linear(hidden_dim, output_dim)


  def forward(self, src, trg):
    embedded_src = self.encoder_embed(src)
    _, (hidden,cell) = self.encoder_lstm(embedded_src)

    embedded_trg = self.decoder_embed(trg[:,:-1])
    outputs, _ = self.decoder_lstm(embedded_trg, (hidden,cell))
    predictions = self.fc_out(outputs)
    return predictions


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Seq2Seq(len(eng_vocab), len(hin_vocab)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

def train_epoch():
  model.train()
  total_loss = 0
  for src, trg in train_loader:
    src, trg = src.to(device), trg.to(device)
    optimizer.zero_grad()
    output = model(src, trg)
    output_dim = output.shape[-1]
    output = output.contiguous().view(-1, output_dim)
    trg = trg[:,1:].contiguous().view(-1)
    loss = criterion(output, trg)
    loss.backward()
    optimizer.step()
    total_loss += loss.item()
  return total_loss / len(train_loader)
