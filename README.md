# Machine-Translation-Basic
Machine Translation (MT) refers to the use of computational models and algorithms to automatically translate text or speech from one language to another. It is a core task in Natural Language Processing (NLP) and has evolved from rule-based systems to statistical approaches and now to neural machine translation (NMT) powered by deep learning.

## Types of Machine Translation Approaches
### 1. Rule-Based Machine Translation (RBMT)

* Relies on linguistic rules (syntax, morphology, grammar).
* Uses bilingual dictionaries and hand-crafted rules.
* Example: Early MT systems like SYSTRAN.
* Advantage: Transparent and explainable translations.
* Limitation: Requires extensive linguistic resources; lacks scalability.

### 2. Statistical Machine Translation (SMT)

* Based on probabilistic models derived from large bilingual corpora.
* Translations are generated by maximizing the probability of the target sentence given the source sentence.
* Key models:
  * Word-based models
  * Phrase-based SMT (PB-SMT)
  * Hierarchical and syntax-based SMT
* Example: Moses Toolkit.
* Advantage: Data-driven, more scalable.
* Limitation: Struggles with long-distance dependencies, word reordering.

### 3. Neural Machine Translation (NMT)

* Uses deep learning models (RNNs, LSTMs, GRUs, Transformers).
* Treats translation as a sequence-to-sequence (seq2seq) problem.
* Attention mechanisms (Bahdanau, Luong) and Transformers (Vaswani et al., 2017) revolutionized MT.
* Examples: Google Translate (modern version), OpenNMT, MarianNMT, HuggingFace Transformers.
* Advantage: Produces fluent, context-aware translations.
* Limitation: Requires large datasets and high computational power.

## Core Concepts in MT

* Parallel Corpus: Aligned bilingual text data used for training.
* Alignment Models: Map words/phrases in the source to the target language.
* Language Model (LM): Ensures fluency in the target language.
* Encoder-Decoder Architecture: NMT framework for handling variable-length sequences.
* Attention Mechanism: Helps models focus on relevant parts of the input sequence.
* Transformers: Current state-of-the-art architecture for MT.

## Evaluation Metrics

* BLEU (Bilingual Evaluation Understudy) – measures n-gram overlap.
* METEOR – considers synonyms and stemming.
* TER (Translation Edit Rate) – measures edits required.
* ChrF++ – character-based F-score for morphologically rich languages.

## Applications

* Global communication (Google Translate, DeepL).
* International business and e-commerce.
* Cross-lingual information retrieval.
* Healthcare and government translation services.
* Real-time speech translation (Zoom, Skype, Microsoft Translator).
